from pyspark.ml.feature import CountVectorizer
from pyspark.ml.feature import HashingTF, IDF
from pyspark.sql.functions import col, concat_ws
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.feature import NGram
from pyspark.ml.feature import Word2Vec
from pyspark.ml import Pipeline


def extract_features(cleaned_tweet_df,
                     feature_data_path=None,
                     feature_pipeline_path=None):

    hashing = HashingTF(inputCol="lemmatized_text", outputCol="raw_token")

    tf_idf = IDF(inputCol="raw_token", outputCol="idf_features")
    count_vectorizer = CountVectorizer(inputCol="lemmatized_text",
                                       outputCol="cv_features",
                                       minDF=1.0)

    bi_gram = NGram(n=2, inputCol="lemmatized_text", outputCol="bi_grams")
    hashing_bi_gram = HashingTF(inputCol="bi_grams",
                                outputCol="hashing_bi_gram_raw_token")

    count_vectorizer_bigram = CountVectorizer(inputCol="bi_grams",
                                              outputCol="cv_bi_gram_features",
                                              minDF=1.0)

    word2_vectorizer = Word2Vec(vectorSize=50,
                                minCount=0,
                                inputCol="lemmatized_text",
                                outputCol="word2_vec_features")

    feature_assembled_vectors = VectorAssembler(
        inputCols=['idf_features', 'cv_features'],
        outputCol='assembled_features')

    tweet_feature_exraction_pipeline = Pipeline(stages=[
        hashing, tf_idf, count_vectorizer, bi_gram, word2_vectorizer,
        hashing_bi_gram, count_vectorizer_bigram, feature_assembled_vectors
    ])

    tweet_feature_exraction_model = tweet_feature_exraction_pipeline.fit(
        cleaned_tweet_df)

    tweet_feature_extracted_df = tweet_feature_exraction_model.transform(
        cleaned_tweet_df)

    if feature_pipeline_path:
        tweet_feature_exraction_model.write().overwrite().save(
            feature_pipeline_path)

    if feature_data_path:
        tweet_feature_extracted_df.write.format("delta").mode(
            "overwrite").save(feature_data_path)

    return tweet_feature_extracted_df


if __name__ == "__main__":
    import os
    import sys
    from dotenv import load_dotenv

    sys.path.append(os.path.join(os.path.dirname(__file__), ".."))
    load_dotenv()

    import be.spark_session_builder as spark_session_builder

    cleaned_tweet_path = os.environ['cleaned_tweet_table_path']
    feature_tweet_path = os.environ['feature_tweet_table_path']
    feature_pipeline_path = os.environ['tweet_feature_pipeline']

    spark = spark_session_builder.build()
    preprocessed_tweet_df = spark.read.format("delta").load(cleaned_tweet_path)
    print(preprocessed_tweet_df.columns)
    df = extract_features(preprocessed_tweet_df, feature_tweet_path,
                          feature_pipeline_path)
    df.select('text', 'lemmatized_text').show(truncate=False)
    print(df.columns)