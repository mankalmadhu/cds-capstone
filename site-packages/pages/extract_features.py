import os
import streamlit as st
from be.tweet_feature_extractor import extract_features
import be.spark_session_builder as spark_session_builder
from pyspark.ml import PipelineModel
import sys


def render_page():
    st.title('Data Feature Extractor')
    st.markdown("Extract features from data")

    cleaned_tweet_path = os.environ['cleaned_tweet_table_path']
    feature_tweet_path = os.environ['feature_tweet_table_path']
    feature_pipeline_path = os.environ['tweet_feature_pipeline']

    spark = spark_session_builder.build()
    preprocessed_tweet_df = spark.read.format("delta").load(cleaned_tweet_path)
    df = extract_features(preprocessed_tweet_df, feature_tweet_path,
                          feature_pipeline_path)

    delta_read_df = spark.read.format("delta").load(feature_tweet_path)
    pipeline_model = PipelineModel.load(feature_pipeline_path)

    st.write(f'Count of tweets {delta_read_df.count()}')
    st.dataframe(delta_read_df.toPandas())
    st.write(pipeline_model.stages)
