import os
import streamlit as st
from be.tweet_feature_extractor import extract_features
import be.spark_session_builder as spark_session_builder
from pyspark.ml import PipelineModel
import sys


def render_page():
    st.title('Data Feature Extractor')
    st.markdown("Extract features from data")

    cleaned_tweet_path = os.environ['cleaned_tweet_table_path']
    feature_tweet_path = os.environ['feature_tweet_table_path']
    feature_pipeline_path = os.environ['tweet_feature_pipeline']

    spark = spark_session_builder.build()
    preprocessed_tweet_df = spark.read.format("delta").load(cleaned_tweet_path)

    extract_live_features = st.checkbox('Run Feature Extraction Pipeline ?')

    if extract_live_features:
        df = extract_features(preprocessed_tweet_df, feature_tweet_path,
                              feature_pipeline_path)

    delta_read_df = spark.read.format("delta").load(feature_tweet_path)
    pipeline_model = PipelineModel.load(feature_pipeline_path)

    df = delta_read_df.select('text', 'cv_features', 'idf_features',
                              'word2_vec_features').toPandas()
    df = df.astype(str)

    st.write(f'Count of tweets {delta_read_df.count()}')
    st.dataframe(df)
    st.write(pipeline_model.stages)
